{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad331a3e",
   "metadata": {},
   "source": [
    "### Loading the data in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e327045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load the data from the hdf5 file\n",
    "def load_h5py_file(file_path):\n",
    "    data = {\n",
    "        'neural_features': [],\n",
    "        'n_time_steps': [],\n",
    "        'seq_class_ids': [],\n",
    "        'seq_len': [],\n",
    "        'transcriptions': [],\n",
    "        'sentence_label': [],\n",
    "        'session': [],\n",
    "        'block_num': [],\n",
    "        'trial_num': [],\n",
    "    }\n",
    "    # Open the hdf5 file for that day\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "\n",
    "        keys = list(f.keys())\n",
    "\n",
    "        # For each trial in the selected trials in that day\n",
    "        for key in keys:\n",
    "            g = f[key]\n",
    "\n",
    "            neural_features = g['input_features'][:]\n",
    "            n_time_steps = g.attrs['n_time_steps']\n",
    "            seq_class_ids = g['seq_class_ids'][:] if 'seq_class_ids' in g else None\n",
    "            seq_len = g.attrs['seq_len'] if 'seq_len' in g.attrs else None\n",
    "            transcription = g['transcription'][:] if 'transcription' in g else None\n",
    "            sentence_label = g.attrs['sentence_label'][:] if 'sentence_label' in g.attrs else None\n",
    "            session = g.attrs['session']\n",
    "            block_num = g.attrs['block_num']\n",
    "            trial_num = g.attrs['trial_num']\n",
    "\n",
    "            data['neural_features'].append(neural_features)\n",
    "            data['n_time_steps'].append(n_time_steps)\n",
    "            data['seq_class_ids'].append(seq_class_ids)\n",
    "            data['seq_len'].append(seq_len)\n",
    "            data['transcriptions'].append(transcription)\n",
    "            data['sentence_label'].append(sentence_label)\n",
    "            data['session'].append(session)\n",
    "            data['block_num'].append(block_num)\n",
    "            data['trial_num'].append(trial_num)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a19a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/t15_copyTask_neuralData/hdf5_data_final'\n",
    "\n",
    "# Recursively search for all *train.hdf5 files under all subdirectories of DATA_PATH\n",
    "def get_data_files(data_type='train'):\n",
    "    \"\"\"\n",
    "    Return a list of files matching the given data type ('train', 'valid', or 'test').\n",
    "    \"\"\"\n",
    "    return glob.glob(os.path.join(DATA_PATH, '**', f'*{data_type}.hdf5'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaef29e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.03/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.04/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.01/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.14/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.05.10/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.28/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.08/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.15/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.10/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.20/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.13/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.25/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.30/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.02.25/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.17/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.24/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.08/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.26/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.19/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.06.14/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.29/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.16/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.17/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.03/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.10/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.08/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.25/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.01/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.06/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.03/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.18/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.13/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.27/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.19/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.21/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.20/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.22/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.28/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.12/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.15/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.17/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.03/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.29/data_train.hdf5',\n",
       " 'data/t15_copyTask_neuralData/hdf5_data_final/t15.2025.04.13/data_train.hdf5']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = get_data_files('train')\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94cfad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data into a single DataFrame\n",
    "def load_data(files):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for file in tqdm(files, desc=\"Loading data\"):\n",
    "        data = load_h5py_file(file)\n",
    "        df = pd.concat([df, pd.DataFrame(data)], ignore_index=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9797e2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 45/45 [00:57<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff4ecbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neural_features</th>\n",
       "      <th>n_time_steps</th>\n",
       "      <th>seq_class_ids</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>transcriptions</th>\n",
       "      <th>sentence_label</th>\n",
       "      <th>session</th>\n",
       "      <th>block_num</th>\n",
       "      <th>trial_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.23488846, 0.5014211, -0.75813776, -0.5213...</td>\n",
       "      <td>544</td>\n",
       "      <td>[6, 40, 36, 17, 21, 40, 15, 25, 40, 12, 5, 23,...</td>\n",
       "      <td>14</td>\n",
       "      <td>[73, 32, 119, 105, 108, 108, 32, 103, 111, 32,...</td>\n",
       "      <td>I will go around.</td>\n",
       "      <td>t15.2023.11.03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.23703846, 0.3616628, -0.7747983, 0.937466...</td>\n",
       "      <td>641</td>\n",
       "      <td>[6, 40, 2, 22, 40, 31, 4, 20, 17, 24, 40, 31, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>[73, 32, 97, 109, 32, 116, 97, 108, 107, 105, ...</td>\n",
       "      <td>I am talking to my family.</td>\n",
       "      <td>t15.2023.11.03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.23900536, -0.8310198, -0.75461406, -0.489...</td>\n",
       "      <td>694</td>\n",
       "      <td>[17, 31, 40, 17, 38, 40, 21, 33, 20, 17, 24, 4...</td>\n",
       "      <td>22</td>\n",
       "      <td>[73, 116, 32, 105, 115, 32, 108, 111, 111, 107...</td>\n",
       "      <td>It is looking quite hard.</td>\n",
       "      <td>t15.2023.11.03</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.24192314, 0.2851043, -0.7366848, -0.48644...</td>\n",
       "      <td>691</td>\n",
       "      <td>[36, 6, 40, 9, 25, 23, 31, 40, 37, 34, 40, 20,...</td>\n",
       "      <td>19</td>\n",
       "      <td>[87, 104, 121, 32, 100, 111, 110, 39, 116, 32,...</td>\n",
       "      <td>Why don't you come here.</td>\n",
       "      <td>t15.2023.11.03</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.24300373, -0.8231351, -0.71545774, -0.482...</td>\n",
       "      <td>918</td>\n",
       "      <td>[6, 40, 37, 34, 39, 3, 36, 3, 21, 18, 40, 15, ...</td>\n",
       "      <td>29</td>\n",
       "      <td>[73, 32, 117, 115, 117, 97, 108, 108, 121, 32,...</td>\n",
       "      <td>I usually go home by this time.</td>\n",
       "      <td>t15.2023.11.03</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     neural_features  n_time_steps  \\\n",
       "0  [[-0.23488846, 0.5014211, -0.75813776, -0.5213...           544   \n",
       "1  [[-0.23703846, 0.3616628, -0.7747983, 0.937466...           641   \n",
       "2  [[-0.23900536, -0.8310198, -0.75461406, -0.489...           694   \n",
       "3  [[-0.24192314, 0.2851043, -0.7366848, -0.48644...           691   \n",
       "4  [[-0.24300373, -0.8231351, -0.71545774, -0.482...           918   \n",
       "\n",
       "                                       seq_class_ids  seq_len  \\\n",
       "0  [6, 40, 36, 17, 21, 40, 15, 25, 40, 12, 5, 23,...       14   \n",
       "1  [6, 40, 2, 22, 40, 31, 4, 20, 17, 24, 40, 31, ...       24   \n",
       "2  [17, 31, 40, 17, 38, 40, 21, 33, 20, 17, 24, 4...       22   \n",
       "3  [36, 6, 40, 9, 25, 23, 31, 40, 37, 34, 40, 20,...       19   \n",
       "4  [6, 40, 37, 34, 39, 3, 36, 3, 21, 18, 40, 15, ...       29   \n",
       "\n",
       "                                      transcriptions  \\\n",
       "0  [73, 32, 119, 105, 108, 108, 32, 103, 111, 32,...   \n",
       "1  [73, 32, 97, 109, 32, 116, 97, 108, 107, 105, ...   \n",
       "2  [73, 116, 32, 105, 115, 32, 108, 111, 111, 107...   \n",
       "3  [87, 104, 121, 32, 100, 111, 110, 39, 116, 32,...   \n",
       "4  [73, 32, 117, 115, 117, 97, 108, 108, 121, 32,...   \n",
       "\n",
       "                    sentence_label         session  block_num  trial_num  \n",
       "0                I will go around.  t15.2023.11.03          1          0  \n",
       "1       I am talking to my family.  t15.2023.11.03          1          1  \n",
       "2        It is looking quite hard.  t15.2023.11.03          1          2  \n",
       "3         Why don't you come here.  t15.2023.11.03          1          3  \n",
       "4  I usually go home by this time.  t15.2023.11.03          1          4  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c9934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data item 0: (544, 512)\n",
      "Shape of data item 1: (641, 512)\n",
      "Shape of data item 2: (694, 512)\n",
      "Shape of data item 3: (691, 512)\n",
      "Shape of data item 4: (918, 512)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the neural features data\n",
    "for i in range(5):\n",
    "    print(f\"Shape of data item {i}: {np.array(train_df['neural_features'][i]).shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
